{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cuda.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeS3ZoAaxYjr",
        "colab_type": "text"
      },
      "source": [
        "# Supercomputing for the Masses\n",
        "\n",
        "This is an html-rendered Jupyter notebook. If you want to do the exercises right here, open it in [Colab]() or [download]() and edit it locally. In former case, you need to complete a little quest and install CUDA and PyCuda, its Python binding. On a Debian-based machine, it will probably be enough to do `apt-get install nvidia-cuda-dev nvidia-cuda-toolkit` and `pip install pycuda`.\n",
        "\n",
        "Prerequisites: basic knowledge of Python and C, basic algorithms, and generally how computers work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq43d5ejxYjs",
        "colab_type": "text"
      },
      "source": [
        "## Subtlties of the Moore's law\n",
        "\n",
        "Here is a graph that roughly represents what is happening in the CPU world:\n",
        "\n",
        "<img width='600px' src='https://www.karlrupp.net/wp-content/uploads/2015/06/35years.png'>\n",
        "\n",
        "**Moore's law** is the observation that the number of transistors in a microprocessor doubles about every two years. This roughly means that the performance doubles too. \n",
        "\n",
        "You can see that around 2005 there became a shift in design .\n",
        "\n",
        "The cores are more or less independent.\n",
        "\n",
        "Modern GPUs appeared in early 2000s. They exploit the specific area they operate.\n",
        "\n",
        "There are physical limitations to the speed of a core.\n",
        "\n",
        "One solid one: the speed of light. You at least need the time for electromagnetic wave (this is light too)  to pass from one side of the motherboard to another."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I46-pYQQxYjt",
        "colab_type": "text"
      },
      "source": [
        "Some of them have"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6dIafXjxYjt",
        "colab_type": "text"
      },
      "source": [
        "The default free GPUs available on Google Colab are [rather powerfull](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-t4/t4-tensor-core-datasheet-951643.pdf). Author has no idea why Google does this, but this is awesome."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJnUIBvYxYju",
        "colab_type": "text"
      },
      "source": [
        "## Why multiprocessing?\n",
        "\n",
        "Clock frequencies — for example, Intel Core i7 can have. This gives an upper bound"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQzOyGWXxYju",
        "colab_type": "text"
      },
      "source": [
        "There are two types of "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0Pedur9xYjv",
        "colab_type": "text"
      },
      "source": [
        "## General-purpose GPU\n",
        "\n",
        "There was a period in time when hedge funds hired computer graphics guys from game companies because of their computing skills.\n",
        "\n",
        "There are several.\n",
        "\n",
        "This is like with Windows and Linux.\n",
        "\n",
        "We will stick with CUDA, because it more spread, especially in fields where noone cares, like deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfwy-HNj_MYY",
        "colab_type": "text"
      },
      "source": [
        "## Heterogineous computing\n",
        "\n",
        "CUDA programming involves running code on two different platforms concurrently: a host system with one or more CPUs and one or more  GPUs.\n",
        "\n",
        "## Differences from CPUs\n",
        "\n",
        "### Threads\n",
        "\n",
        "Threads on a CPU are generally heavyweight entities. The operating system must swap threads on and off CPU execution channels to provide multithreading capability. Context switches are therefore slow and expensive.\n",
        "\n",
        "By comparison, threads on GPUs are extremely lightweight. In a typical system, thousands of threads are queued up for work — in warps of 32 threads each. If the GPU must wait on one warp of threads, it simply begins executing work on another. Because separate registers are allocated to all active threads, no swapping of registers or other state need occur when switching among GPU threads. Resources stay allocated to each thread until it completes its execution.\n",
        "\n",
        "In short, CPU cores are designed to minimize latency for one or two threads at a time each, whereas GPUs are designed to handle a large number of concurrent, lightweight threads in order to maximize throughput.\n",
        "\n",
        "### Memory\n",
        "\n",
        "The host system and the device each have their own distinct attached physical memories. As the host and device memories are separated by the PCI Express (PCIe) bus, items in the host memory must occasionally be communicated across the bus to the device memory or vice versa as described in What Runs on a CUDA-Enabled Device?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad5CKV2QxYjw",
        "colab_type": "text"
      },
      "source": [
        "You can easily dump 98% of performance of you think this way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-Agg3ULxYjw",
        "colab_type": "text"
      },
      "source": [
        "## Setting up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXLhU1ktxYjy",
        "colab_type": "text"
      },
      "source": [
        "If you are on Colab, go to Runtime -> Change runtime type -> Hardware accelerator and set it to \"GPU\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "128k7rwExYjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# you may want to clear the output of this cell after installation\n",
        "from IPython.display import clear_output\n",
        " \n",
        "# this might take a while\n",
        "!pip install pycuda\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Efm3zMQrxYj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from pycuda.compiler import SourceModule\n",
        "import pycuda.driver as drv\n",
        "import pycuda.autoinit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJt1LrKB6UIe",
        "colab_type": "text"
      },
      "source": [
        "## The basics\n",
        "\n",
        "Let's start with a simple example and then dive deeper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF7tut_hxYj7",
        "colab_type": "text"
      },
      "source": [
        "## Example: $a + b$\n",
        "\n",
        "For testing and coordination with host, we will use **NumPy** package. If you don't have it, install it: `pip install numpy`.\n",
        "\n",
        "NumPy is a package for linear algebra and array manupulation in Python. It is written in C and is very efficient, but runs solely on CPU, so we will benchmark against it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFzrtEGe6u6F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "5d223ddb-de1a-4572-8937-e46801122bb1"
      },
      "source": [
        "# lets generate our test data: two float arrays filled with something random\n",
        "a = numpy.random.randn(100).astype('float32') \n",
        "b = numpy.random.randn(100).astype('float32')\n",
        "# the type needs to be specified in this case, because randn's default type is float64, but CUDA knows nothing about it\n",
        "\n",
        "# we need to create space where kernel should write its answers to\n",
        "dest = numpy.zeros_like(a)\n",
        "\n",
        "# this is the kernel itself\n",
        "mod = SourceModule(\"\"\"\n",
        "    __global__ void add(float *dest, float *a, float *b) {\n",
        "      const int i = threadIdx.x;\n",
        "      dest[i] = a[i] + b[i];\n",
        "    }\n",
        "\"\"\")\n",
        "\n",
        "# you need to specify the source code, and PyCUDA will compile it\n",
        "add_kernel = mod.get_function(\"add\")\n",
        "\n",
        "add_kernel(\n",
        "    drv.Out(dest),  # specifies that this memory should be accessible for writing\n",
        "    drv.In(a),  # specifies this should be accessible for reading\n",
        "    drv.In(b),\n",
        "    block=(100,1,1)  # we'll talk about it in a minute\n",
        ")\n",
        "\n",
        "assert np.allclose(dest, a + b), 'WA'  # checks that these are equal\n",
        "print('OK')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-afc857479fe4>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    %%time\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXp0OEvt9hBb",
        "colab_type": "text"
      },
      "source": [
        "### Memory management\n",
        "\n",
        "In CUDA C API, you need to allocate memory explicitly. So this is actually really nice.\n",
        "\n",
        "There is also `drv.InOut` function, which makes it available for both reading and writing, but we won't use it in this tutorial because we need to test our code too.\n",
        "\n",
        "Most of the operations here are memory operations, so measuring performance here is useless. Don't worry, we will get to more complex examples soon enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6lk0SHExYj5",
        "colab_type": "text"
      },
      "source": [
        "GPUs have very specific operations. However, in case of NVIDIA GPUs managing it is quite simple: the cards have *compute capabilities* (1.0, 1.1, 1.2, 1.3, 2.0, etc.) and all features added at capability $x$ is also available at later versions. These can be checked at run-time or compile-time.\n",
        "\n",
        "You can check differences in this Wikipedia article: https://en.wikipedia.org/wiki/CUDA#Version_features_and_specifications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krsUxXBY-SIx",
        "colab_type": "text"
      },
      "source": [
        "## Synchronization\n",
        "\n",
        "**Reduction** is any array-wise operation.\n",
        "\n",
        "Assume the following problem:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_kC651RB7CH",
        "colab_type": "text"
      },
      "source": [
        "## Work vs. Latency\n",
        "\n",
        "We actually think about both work and step complexity now.\n",
        "\n",
        "Some tasks, especially in cryptography, cannot be parallelized. But some can."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6apWv7QA4ZL",
        "colab_type": "text"
      },
      "source": [
        "## Example: summing array in $O(\\log n)$ time\n",
        "\n",
        "Assume we want to perform some associative operation on an array of $n$ elements. Say, sum it up.\n",
        "\n",
        "<img width='400px' src='http://i.stack.imgur.com/Uehc3.png'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmEssYVWA3lv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCUdNtVBxYj6",
        "colab_type": "text"
      },
      "source": [
        "## Why CUDA\n",
        "\n",
        "Most of it still applicable.\n",
        "\n",
        "Again, GPU programming is very specific.\n",
        "\n",
        "SSE and tensor cores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oAIePrXxYj6",
        "colab_type": "text"
      },
      "source": [
        "## Kernels\n",
        "\n",
        "Just like C or C++, except that you use some custom built-in functions and specifiers.\n",
        "\n",
        "CUDA is pretty much like normal C, except that you can specify some functions to be run as. Depending on implementation, the workflow goes as follows:\n",
        "\n",
        "You need to think of your computer as a heterogenious machine: there is host data and device data.\n",
        "\n",
        "* You move input data to device memory.\n",
        "* You run some computation on device.\n",
        "* You retrieve back this data.\n",
        "\n",
        "In fact, kernel runs are concurrent — you program does not block until kernel run is complete. Newer devices can even run multiple kernels concurrently this way and wait for their results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v47t8e01xYkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mod = SourceModule(\"\"\"\n",
        "  __global__ void check_prime(int *a, int *b, int *c) {\n",
        "    int n = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "    for (int i = 2; i < 10000; i++)\n",
        "      a[i] |= (n % i == 0);\n",
        "  }\n",
        "\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJlYS_mMxYkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = numpy.zeros(10**8, dtype=numpy.int)\n",
        "\n",
        "func = mod.get_function(\"check_prime\")\n",
        "func(cuda.InOut(a), block=(len(a)//32,32, 1))\n",
        "\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxVitXK8xYkH",
        "colab_type": "text"
      },
      "source": [
        "What you need to understand about GPUs is that they are extremely specialised for their applications.\n",
        "\n",
        "Intrinsics for that.\n",
        "\n",
        "Now, a lot of value comes from cryptocurrency and deep learning. The latter relies on two specific operations: matrix multiplications for linear layers and convolutions for convolutional layers used in computer vision.\n",
        "\n",
        "First, they introduced \"multiply-accumulate\" operation (e. g. `x += y * z`) per 1 GPU clock cycle.\n",
        "\n",
        "Google uses Tensor Processing Units. Nobody really knows how they work (proprietary hardware that they rent, not sell).\n",
        "\n",
        "Each tensor core perform operations on small matrices with size 4x4. Each tensor core can perform 1 matrix multiply-accumulate operation per 1 GPU clock. It multiplies two fp16 matrices 4x4 and adds the multiplication product fp32 matrix (size: 4x4) to accumulator (that is also fp32 4x4 matrix).\n",
        "\n",
        "This is a lot of work per \n",
        "\n",
        "Well, you don't really need anything more precise than that for deep learning anyway.\n",
        "\n",
        "\n",
        "It is called mixed precision because input matrices are fp16 but multiplication result and accumulator are fp32 matrices.\n",
        "\n",
        "Probably, the proper name would be \"4x4 matrix cores\", however NVIDIA marketing team decided to use \"tensor cores\".\n",
        "\n",
        "So, see, this is not exactly fair comparison.\n",
        "\n",
        "<img width='500px' src='https://static.seekingalpha.com/uploads/2018/8/11/275308-15340093003448672_origin.png'>\n",
        "\n",
        "*<center>You need to extend this graph just a little bit: last November, NVIDIA's stock dropped 30% following Bitcoin crash, so I wouldn't be so hopefull</center>*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN8sBLR0xYkH",
        "colab_type": "text"
      },
      "source": [
        "down to int4 (16-valued, you heard correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RvqW0ZjxYkI",
        "colab_type": "text"
      },
      "source": [
        "You need to know a lot of this specialised stuff to write efficient code. So this is a bad idea to write libraries from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY1clJmvxYkI",
        "colab_type": "text"
      },
      "source": [
        "Anyway, for pedagogical and recreational reasons, today we will reinvent the wheel and do a matrix multiply."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOrwQ3wrxYkJ",
        "colab_type": "text"
      },
      "source": [
        "## Warps and thread blocks\n",
        "\n",
        "Threads are bandled in groups of 32. All threads in a group must be either waiting or performing the same operation. This is caused by architectural difficulties.\n",
        "\n",
        "<img width='300px' src='https://upload.wikimedia.org/wikipedia/commons/thumb/5/5b/Block-thread.svg/1920px-Block-thread.svg.png'>\n",
        "\n",
        "You can actually do the same stuff with 2d and 3d indexing — weird, right?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc7yOY85xYkK",
        "colab_type": "text"
      },
      "source": [
        "## Reducing an array\n",
        "\n",
        "It seems to be simple: you just need to .\n",
        "\n",
        "What actually happens when you do `s += x`? This is not a single operation. Actually, four things happen:\n",
        "\n",
        "1. Read $x$ into register\n",
        "2. Read $s$ into register\n",
        "3. Calculate $s + x$\n",
        "4. Write it back to wherever $s$ was initially\n",
        "\n",
        "Two threads may execute it in an interleaved fashion. Say thread A could get $s$, but a nanosecond later thread B will be writing here, but thread A doesn't know about it and will re-write unchanged value.\n",
        "\n",
        "\n",
        "\n",
        "Note: Atomics to do that\n",
        "\n",
        "for small data types they are implemented on the hardware level and much more faster than that.\n",
        "\n",
        "std::atomic is introduced to handle atomic operations in multi-thread context. In multi-thread environment, when two threads operating on the same variable, you must be extra careful to avoid race conditions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiXbo9eOxYkK",
        "colab_type": "text"
      },
      "source": [
        "## Memory types\n",
        "\n",
        "If all the various types of device memory were to race, here’s how the race would turn out:\n",
        "\n",
        "Register size (= machine word width) is 32 bits, but they also contain 64bit capability (otherwise having more than 4gb of memory would not be possible).\n",
        "\n",
        "* 1st place: **Register memory**\n",
        "  <br> This is the data visible only to the thread that wrote it. It lasts only for the lifetime of that thread.\n",
        "* 2nd place: **Shared Memory**\n",
        "  <br> Shared to all threads within thread block. Lasts only for the lifetime of that block. This type of memory allows communication (data sharing) between threads. This is why you should\n",
        "* 3rd place: **Constant Memory**\n",
        "  <br> \n",
        "* 4th: Texture Memory\n",
        "* Tie for last place: Local Memory and Global Memory\n",
        "\n",
        "What you need to care for now is register \n",
        "\n",
        "For now, you need to care about differe\n",
        "\n",
        "Accessing global memory takes hundreds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qZG7IGPxYkL",
        "colab_type": "text"
      },
      "source": [
        "## Problem: dense matrix multiplication\n",
        "\n",
        "A lot of these are actually sparse. You can do stuff with social network graphs or web graphs.\n",
        "\n",
        "Cool. But let's disapploint us for a bit:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kla1qGaLxYkN",
        "colab_type": "text"
      },
      "source": [
        "## Problem: dynamic programming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNnOYae3xYkN",
        "colab_type": "text"
      },
      "source": [
        "## Some primitives\n",
        "\n",
        "Before we go deeper into more complex algorithms, we need to master some "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfXYZd8RxYkO",
        "colab_type": "text"
      },
      "source": [
        "## Problem: sorting"
      ]
    }
  ]
}