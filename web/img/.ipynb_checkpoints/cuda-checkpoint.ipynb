{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supercomputing for the Masses\n",
    "\n",
    "This is htmt-rendered notebook is supposed to be opened with Colab or a debian-based linux machine with a CUDA-capable GPU.\n",
    "\n",
    "You need to complete a quest and install CUDA.\n",
    "\n",
    "We do not expect anything from reader except some knowledge of C, Python basic algorithms and generally how computers work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtlties of the Moore's law\n",
    "\n",
    "There are physical limitations to the speed of processors.\n",
    "\n",
    "One solid one: the speed of light. You at least need the time for electromagnetic wave (this is light too)  to pass from one side of the motherboard to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of them have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default free GPUs available on Google Colab are [rather powerfull](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-t4/t4-tensor-core-datasheet-951643.pdf). Author has no idea why Google does this, but this is awesome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why multiprocessing?\n",
    "\n",
    "Clock frequencies — for example, Intel Core i7 can have. This gives an upper bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General-purpose GPU\n",
    "\n",
    "There was a period in time when hedge funds hired computer graphics guys from game companies because of their computing skills.\n",
    "\n",
    "There are several.\n",
    "\n",
    "This is like with Windows and Linux.\n",
    "\n",
    "We will stick with CUDA, because it more spread, especially in fields where noone cares, like deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily dump 98% of performance of you think this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything should work if you're on Colab. Otherwise, you might need to complete a [little quest](https://wiki.tiker.net/PyCuda/Installation/Linux/Ubuntu). It will probably be enough to do `sudo apt-get install nvidia-cuda-dev` (install CUDA itself) and then return to `pip install pycuda`.\n",
    "\n",
    "You can get it by running `lspci | grep NVIDIA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "from pycuda.compiler import SourceModule\n",
    "import pycuda.driver as drv\n",
    "import pycuda.autoinit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUs have very specific operations. However, in case of NVIDIA GPUs managing it is quite simple: the cards have *compute capabilities* (1.0, 1.1, 1.2, 1.3, 2.0, etc.) and all features added at capability $x$ is also available at later versions. These can be checked at run-time or compile-time.\n",
    "\n",
    "You can check differences in this Wikipedia article: https://en.wikipedia.org/wiki/CUDA#Version_features_and_specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why CUDA\n",
    "\n",
    "Most of it still applicable.\n",
    "\n",
    "Again, GPU programming is very specific.\n",
    "\n",
    "SSE and tensor cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels\n",
    "\n",
    "Just like C or C++, except that you use some custom built-in functions and specifiers.\n",
    "\n",
    "CUDA is pretty much like normal C, except that you can specify some functions to be run as. Depending on implementation, the workflow goes as follows:\n",
    "\n",
    "You need to think of your computer as a heterogenious machine: there is host data and device data.\n",
    "\n",
    "* You move input data to device memory.\n",
    "* You run some computation on device.\n",
    "* You retrieve back this data.\n",
    "\n",
    "In fact, kernel runs are concurrent — you program does not block until kernel run is complete. Newer devices can even run multiple kernels concurrently this way and wait for their results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: $a + b$\n",
    "\n",
    "For coordination and testing, we will use `numpy` package. If you don't have it, install it: `pip install numpy`. This is also very optimized, so we will benchmark against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "LaunchError",
     "evalue": "cuMemAlloc failed: unspecified launch failure",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLaunchError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-500b25c44181>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0ma_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem_alloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemcpy_htod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLaunchError\u001b[0m: cuMemAlloc failed: unspecified launch failure"
     ]
    }
   ],
   "source": [
    "a = numpy.random.randn(4,4)\n",
    "\n",
    "a = a.astype(numpy.float32)\n",
    "\n",
    "a_gpu = cuda.mem_alloc(a.size * a.dtype.itemsize)\n",
    "\n",
    "cuda.memcpy_htod(a_gpu, a)\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "    __global__ void doublify(float *a)\n",
    "    {\n",
    "      int idx = threadIdx.x + threadIdx.y*4;\n",
    "      a[idx] *= 2;\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "func = mod.get_function(\"doublify\")\n",
    "func(a_gpu, block=(4,4,1))\n",
    "\n",
    "a_doubled = numpy.empty_like(a)\n",
    "cuda.memcpy_dtoh(a_doubled, a_gpu)\n",
    "print(a)\n",
    "print(a_doubled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "LaunchError",
     "evalue": "cuModuleLoadDataEx failed: unspecified launch failure - ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLaunchError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d23febd99c6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0mdest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     }\n\u001b[0;32m----> 6\u001b[0;31m \"\"\")\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0madd_kernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"add\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pycuda/compiler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source, nvcc, options, keep, no_extern_c, arch, code, cache_dir, include_dirs)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpycuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_from_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_from_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcubin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bind_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLaunchError\u001b[0m: cuModuleLoadDataEx failed: unspecified launch failure - "
     ]
    }
   ],
   "source": [
    "mod = SourceModule(\"\"\"\n",
    "    __global__ void add(float *dest, float *a, float *b) {\n",
    "      const int i = threadIdx.x;\n",
    "      dest[i] = a[i] + b[i];\n",
    "    }\n",
    "\"\"\")\n",
    "\n",
    "add_kernel = mod.get_function(\"add\")\n",
    "\n",
    "a = numpy.random.randn(400).astype(numpy.float32)\n",
    "b = numpy.random.randn(400).astype(numpy.float32)\n",
    "\n",
    "dest = numpy.zeros_like(a)\n",
    "\n",
    "add_kernel(drv.Out(dest), drv.In(a), drv.In(b), block=(400,1,1))\n",
    "\n",
    "print(dest-a*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = SourceModule(\"\"\"\n",
    "  __global__ void check_prime(int *a, int *b, int *c) {\n",
    "    int n = blockIdx.x*blockDim.x + threadIdx.x;\n",
    "    for (int i = 2; i < 10000; i++)\n",
    "      a[i] |= (n % i == 0);\n",
    "  }\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = numpy.zeros(10**8, dtype=numpy.int)\n",
    "\n",
    "func = mod.get_function(\"check_prime\")\n",
    "func(cuda.InOut(a), block=(len(a)//32,32, 1))\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you need to understand about GPUs is that they are extremely specialised for their applications.\n",
    "\n",
    "Intrinsics for that.\n",
    "\n",
    "Now, a lot of value comes from cryptocurrency and deep learning. The latter relies on two specific operations: matrix multiplications for linear layers and convolutions for convolutional layers used in computer vision.\n",
    "\n",
    "First, they introduced \"multiply-accumulate\" operation (e. g. `x += y * z`) per 1 GPU clock cycle.\n",
    "\n",
    "Google uses Tensor Processing Units. Nobody really knows how they work (proprietary hardware that they rent, not sell).\n",
    "\n",
    "Each tensor core perform operations on small matrices with size 4x4. Each tensor core can perform 1 matrix multiply-accumulate operation per 1 GPU clock. It multiplies two fp16 matrices 4x4 and adds the multiplication product fp32 matrix (size: 4x4) to accumulator (that is also fp32 4x4 matrix).\n",
    "\n",
    "This is a lot of work per \n",
    "\n",
    "Well, you don't really need anything more precise than that for deep learning anyway.\n",
    "\n",
    "\n",
    "It is called mixed precision because input matrices are fp16 but multiplication result and accumulator are fp32 matrices.\n",
    "\n",
    "Probably, the proper name would be \"4x4 matrix cores\", however NVIDIA marketing team decided to use \"tensor cores\".\n",
    "\n",
    "So, see, this is not exactly fair comparison.\n",
    "\n",
    "<img width='500px' src='https://static.seekingalpha.com/uploads/2018/8/11/275308-15340093003448672_origin.png'>\n",
    "\n",
    "*<center>You need to extend this graph just a little bit: last November, NVIDIA's stock dropped 30% following Bitcoin crash, so I wouldn't be so hopefull</center>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "down to int4 (16-valued, you heard correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to know a lot of this specialised stuff to write efficient code. So this is a bad idea to write libraries from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway, for pedagogical and recreational reasons, today we will reinvent the wheel and do a matrix multiply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warps and thread blocks\n",
    "\n",
    "Threads are bandled in groups of 32. All threads in a group must be either waiting or performing the same operation. This is caused by architectural difficulties.\n",
    "\n",
    "<img width='300px' src='https://upload.wikimedia.org/wikipedia/commons/thumb/5/5b/Block-thread.svg/1920px-Block-thread.svg.png'>\n",
    "\n",
    "You can actually do the same stuff with 2d and 3d indexing — weird, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing an array\n",
    "\n",
    "It seems to be simple: you just need to .\n",
    "\n",
    "What actually happens when you do `s += x`? This is not a single operation. Actually, four things happen:\n",
    "\n",
    "1. Read $x$ into register\n",
    "2. Read $s$ into register\n",
    "3. Calculate $s + x$\n",
    "4. Write it back to wherever $s$ was initially\n",
    "\n",
    "Two threads may execute it in an interleaved fashion. Say thread A could get $s$, but a nanosecond later thread B will be writing here, but thread A doesn't know about it and will re-write unchanged value.\n",
    "\n",
    "\n",
    "\n",
    "Note: Atomics to do that\n",
    "\n",
    "for small data types they are implemented on the hardware level and much more faster than that.\n",
    "\n",
    "std::atomic is introduced to handle atomic operations in multi-thread context. In multi-thread environment, when two threads operating on the same variable, you must be extra careful to avoid race conditions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory types\n",
    "\n",
    "If all the various types of device memory were to race, here’s how the race would turn out:\n",
    "\n",
    "Register size (= machine word width) is 32 bits, but they also contain 64bit capability (otherwise having more than 4gb of memory would not be possible).\n",
    "\n",
    "* 1st place: **Register memory**\n",
    "  <br> This is the data visible only to the thread that wrote it. It lasts only for the lifetime of that thread.\n",
    "* 2nd place: **Shared Memory**\n",
    "  <br> Shared to all threads within thread block. Lasts only for the lifetime of that block. This type of memory allows communication (data sharing) between threads. This is why you should\n",
    "* 3rd place: **Constant Memory**\n",
    "  <br> \n",
    "* 4th: Texture Memory\n",
    "* Tie for last place: Local Memory and Global Memory\n",
    "\n",
    "What you need to care for now is register \n",
    "\n",
    "For now, you need to care about differe\n",
    "\n",
    "Accessing global memory takes hundreds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heterogineous computing\n",
    "\n",
    "CUDA programming involves running code on two different platforms concurrently: a host system with one or more CPUs and one or more CUDA-enabled NVIDIA GPU devices.\n",
    "\n",
    "Threading resources\n",
    "Execution pipelines on host systems can support a limited number of concurrent threads. Servers that have four hex-core processors today can run only 24 threads concurrently (or 48 if the CPUs support Hyper-Threading.) By comparison, the smallest executable unit of parallelism on a CUDA device comprises 32 threads (termed a warp of threads). Modern NVIDIA GPUs can support up to 1536 active threads concurrently per multiprocessor (see Features and Specifications of the CUDA C Programming Guide) On GPUs with 16 multiprocessors, this leads to more than 24,000 concurrently active threads.\n",
    "Threads\n",
    "Threads on a CPU are generally heavyweight entities. The operating system must swap threads on and off CPU execution channels to provide multithreading capability. Context switches (when two threads are swapped) are therefore slow and expensive. By comparison, threads on GPUs are extremely lightweight. In a typical system, thousands of threads are queued up for work (in warps of 32 threads each). If the GPU must wait on one warp of threads, it simply begins executing work on another. Because separate registers are allocated to all active threads, no swapping of registers or other state need occur when switching among GPU threads. Resources stay allocated to each thread until it completes its execution. In short, CPU cores are designed to minimize latency for one or two threads at a time each, whereas GPUs are designed to handle a large number of concurrent, lightweight threads in order to maximize throughput.\n",
    "RAM\n",
    "The host system and the device each have their own distinct attached physical memories. As the host and device memories are separated by the PCI Express (PCIe) bus, items in the host memory must occasionally be communicated across the bus to the device memory or vice versa as described in What Runs on a CUDA-Enabled Device?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: dense matrix multiplication\n",
    "\n",
    "A lot of these are actually sparse. You can do stuff with social network graphs or web graphs.\n",
    "\n",
    "Cool. But let's disapploint us for a bit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some primitives\n",
    "\n",
    "Before we go deeper into more complex algorithms, we need to master some "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: sorting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
