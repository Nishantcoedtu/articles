# Пересчёт динамики по слоям

В этой задаче мы рассмотрим 4 связанных между собой способа оптимизации динамики. Во всех четырёх мы будем решать одну и ту же задачу:

> Даны $n$ точек на прямой. Нужно найти $m$ отрезков, покрывающих все точки, минимизировав при этом сумму квадратов их длин.

Базовое решение — это следующая динамика:

* $f[i, j]$ — минимальная стоимость покрытия $i$ первых (самых левых) точек, используя не более $j$ отрезков.

* Переход — перебор всех возможных последних отрезков, то есть $f[i, j] = \min_{k < i} \{f[k, j-1] + (x_{i-1}-x_k)^2 \}$. Индексация $x_i$ с нуля.

Итоговый ответ будет записан в $f[n, m]$, а такое решение непосредственным перебором будет работать за $O(n^2 m)$.

```c++
// x[] — отсортированный массив координат точек, нумерация с нуля

// квадрат длины отрезка от i-той до j-той точки
int cost(int i, int j) { return (x[j]-x[i])*(x[j]-x[i]); }

// TODO: предподсчитать cost

for (int i = 0; i <= m; i++)
    f[0][k] = 0; // если нам не нужно ничего покрывать, то всё и так хорошо
// все остальные f предполагаем равными бесконечности

for (int i = 1; i <= n; i++)
    for (int j = 1; j <= m; j++)
        for (int k = 0; k < i; k++)
            f[i][j] = min(f[i][j], f[k][j-1] + cost(k, i-1));
```

Заметим, что циклы по `i` и `j` можно поменять местами.

## Разделяй-и-властвуй

Обозначим за $opt[i, j]$ оптимальный $k$, то есть тот, на котором $f[i, j] = f[k, j-1] + (x_{i-1}-x_k)^2$ минимизируется. Для однозначности, если оптимальный индекс не один, то выберем среди них самый правый.

**Утверждение.** $opt[i, j] \leq opt[i, j+1]$.

Интуиция такая: если у нас появился дополнительный отрезок, то последний отрезок нам не выгодно делать больше.

Что это нам даёт? Если мы уже знаем $opt[i, l]$ и $opt[i, r]$ и хотим посчитать $opt[i, j]$ для какого-то $j$ между $l$ и $r$, то мы можем сузить отрезок поиска оптимального индекса со всего $[0, i-1]$ до $[opt[i, l], opt[i, r]]$.

Будем делать следующее: заведем рекурсивную функцию, которая считает динамики для отрезка $[l, r]$, зная, что их $opt$-ы лежат между $l'$ и $r'$. Она берет середину отрезка $[l, r]$ и линейным проходом считает ответ для неё, а затем просто спускается дальше рекурсивно.

```c++
void solve(int l, int r, int _l, int _r, int k) {
    if (l > r)
        return; // отрезок пустой -- выходим
    int t = (l + r) / 2, opt = _l;
    for (int i = _l; i <= min(_r, t); i++) { 
        int val = f[i+1][k-1] + cost(i, j);
        if (val < f[t][k])
            f[t][k] = val, opt = i;
    }
    solve(l, t-1, _l, opt, k);
    solve(t+1, r, opt, _r, k);
}
```

Затем последовательно вызовем её для каждого слоя:

```c++
for (int k = 1; k <= m; k++)
    solve(0, n-1, 0, n-1, k);
```

**Асимптотика.** Теперь пересчет одного «слоя» динамики занимает $O(n \log n)$ вместо $O(n^2)$, потому что каждый раз рекурсивная функция уменьшает в два раза хотя бы один из отрезков. Так как максимальная глубина рекурсии будет $O(\log n)$, то каждый элемент будет просмотрен не более $O(\log n)$ раз.

Таким образом, мы улучшили асимптотику до $O(n m \log n)$.

## Оптимизация Кнута

Предыдущий метод основывался на том факте, что $opt[i, j] \leq opt[i, j+1]$. А что, если $opt$ монотонен ещё и по первому параметру?

$$ opt[i-1, j] \leq opt[i, j] \leq opt[i, j+1] $$

В задаче это выполняется примерно по той же логике: если наш отрезок стал меньше, то мы можем себе позволить больший последний отрезок.

Давайте теперь просто для каждого состояния перебирать элементы непосредственно от $ opt[i-1, j] $ до $ opt[i, j+1] $. Выясняется, что это работает быстро. Чтобы понять почему, распишем количество элементов, которые мы просмотрим для каждого состояния, и просуммируем:

$$ \sum_{i, j} (opt[i, j+1] - opt[i-1, j] + 1) = nm + \sum_{ij} (opt[i, j+1] - opt[i-1, j]) = O((n+m)n) = O(n^2) $$

Здесь мы заметили, что все элементы, кроме граничных, учитываются в сумме ровно два раза — один раз с плюсом, другой с минусом. Каждый из $opt[i, j]$ не более $O(n)$.

```c++
for (int i = 1; i <= n; i++) {
    for (int j = m; j >= 1; j--) {
        for (int k = opt[i-1][j]; k <= opt[i][j+1]; k++) {
            int val = f[i+1][k-1] + cost(i, j);
            if (val < f[t][k])
                f[t][k] = val, opt[i][j] = i;
        }
    }
}
```

Сравните с базовым решением — всего 3 новых строчки.

## Convex Hull Trick

Этот метод призывает думать об оптимизируемой функции геометрически — посмотрев на `cost` и увидев там скалярное произведение.

$$
f[i, j] = \min_{k < i} \{ f[k, j-1] + (x_{i-1}-x_k)^2 \} = \min_{k < i} \{
f[k, j-1] + x_{i-1}^2
- 2x_{i-1} x_k
+ x_k^2
\}
$$

Посмотрим внимательнее на минимизируемое выражение. $x_{i-1}^2$ не зависит от $k$, значит его можно вынести. Под минимумом останется:

$$
\underbrace{f[k, j-1] + x_k^2}_{a_k}
\underbrace{-2x_k}_{b_k} x_{i-1}
$$

Это теперь можно переписать как $\min_k (a_k, b_k) \cdot (1, X_{i-1})$ (тут имеется в виду скалярное произведение).

Представим $(a_k, b_k)$ как точки на плоскости. Тогда мы можем построить их нижнюю огибающие и бинарным поиском находить оптимальную, с минимальным скалярным произведением.

TODO: иллюстрация.

![Паблик «Mathematical bullshit»](../img/scooby-doo.jpg)

### Дерево Ли Шао

Существует другой подход: увидеть здесь не точки и оптимизацию скалярного произведения, и линии и нахождение минимума в точке.

## Лямбда-оптимизация

**Примечание.** В научной литературе метод известен как дискретный метод множителей Лагранжа.

Рассмотрим немного другую задачу. Пусть нам нужно покрыть те же точки, но теперь нас не ограничивают жёстко в количестве отрезков, а просто штрафуют на какую-то константу $\lambda$ за использование каждого. Нашу оптимизируемую функцию $g$ можно выразить через $f$ следующим образом:

$$
g[i] = \min_{k < i} \{f[i, k] + k \cdot \lambda \}
$$

 Однако её можно считать по более оптимальной формуле, не сводя к вычислению $f$:

$$
g[i] = \lambda + \min_{k < i} \{g[k] + (x_{i-1} - x_k)^2 \}
$$

Эту динамику можно посчитать за $O(n)$ — мы это делали полстраницы назад с помощью Convex Hull Trick.

**Наблюдение 1.** Если в оптимальном решении для $g_i$ мы для какого-то $\lambda$ использовали ровно $k$ отрезков, то это же решение будет оптимальным и для $f[i][k]$.

**Наблюдение 2.** Если уменьшать $\lambda$, то оптимальное количество отрезков для для $g_i$ будет увеличиваться.

Основная идея оптимизации: сделаем бинпоиск по $\lambda$, внутри которого будем находить оптимальное решение для $g_i$ с таким $\lambda$. Если оптимальное $k$ больше $j$, то следующая $\lambda$ должна быть меньше, а в противном случае наоборот. Когда $k$ совпадёт с $j$, просто выведем «чистую» стоимость получившегося решения.

Таким образом, задача решается за $O(n \log n + n \log m)$, если сортировку точек для CHT делать заранее, а не внутри бинпоиска.

Мы не учли только одну деталь: почему вообще существует такая $\lambda$, что оптимальное $k = j$. Возможно, что функция $k(\lambda)$ через него «перескакивает». В общем случае это действительно проблема: одной лишь монотонности не достаточно, чтобы решать подобным образом произвольные задачи с ограничением на число объектов.

**Утверждение.** Функция $f[i, j]$ *нестрого вогнутая* (то есть выпуклая вверх) по своему второму аргументу, то есть:

$$
f[i, j] - f[i, j-1] \leq f[i, j+1] - f[i, j]
$$

Иными словами, «выгода» добавления следующего отрезка с каждым разом не увеличивается.

TODO

## Суммируем

TODO: сделать табличку

* Разделяйка: $O(nm \log n)$, если `cost` такой, что `opt` монотонна по одному аргументу.
* Кнут: $O(nm)$, если `cost` такой, что `opt` монотонна по обоим аргументам.
* CHT: $O(nm)$. В оптимизируемой функции нужно увидеть скалярное произведение.
* Лагранж: $O(n \log n)$. Функция должна быть выпуклой.
